{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Current stable release for CPU and GPU\n",
    "!pip install tensorflow\n",
    "\n",
    "# Or try the preview build (unstable)\n",
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b7ecc",
   "metadata": {},
   "source": [
    "# Importing data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c172d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: YOU SHOULD ONLY NEED TO RUN THIS STEP THE FIRST TIME IN A SESSION\n",
    "import tensorflow as tf\n",
    "\n",
    "# get the data from github and unzip\n",
    "!wget https://raw.githubusercontent.com/andrsn/data/main/speechImageData.zip\n",
    "!unzip -q /content/speechImageData.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ab5f7",
   "metadata": {},
   "source": [
    "# Pre-process data into training and validation sets, using Keras dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='/Users/abdulrasheed/Desktop/Robotics/Deep Learning Assign/Python/speechImageData/TrainData',\n",
    "#     directory='/content/speechImageData - Copy/TrainData',\n",
    "    labels='inferred',\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=128,\n",
    "    image_size=(98, 50)\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='/Users/abdulrasheed/Desktop/Robotics/Deep Learning Assign/Python/speechImageData/ValData',\n",
    "#     directory='/content/speechImageData - Copy/ValData',\n",
    "    labels='inferred',\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='categorical',\n",
    "    batch_size=128,\n",
    "    image_size=(98, 50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f06351",
   "metadata": {},
   "source": [
    "# Extract input-output data into arrays, which can be more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Input, Conv2D, MaxPooling2D, Flatten, Softmax\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "#####\n",
    "\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "# Extract the  training input images and output class labels\n",
    "x_train = []\n",
    "y_train = []\n",
    "for images, labels in train_ds.take(-1):\n",
    "    x_train.append(images.numpy())\n",
    "    y_train.append(labels.numpy())\n",
    "\n",
    "x_train = np.concatenate(x_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "# Extract the validation input images and output class labels\n",
    "x_val = []\n",
    "y_val = []\n",
    "for images, labels in val_ds.take(-1):\n",
    "    x_val.append(images.numpy())\n",
    "    y_val.append(labels.numpy())\n",
    "\n",
    "x_val = np.concatenate(x_val, axis=0)\n",
    "y_val = np.concatenate(y_val, axis=0)\n",
    "\n",
    "bayes_train_x = x_train\n",
    "bayes_train_y = y_train\n",
    "bayes_val_x = x_val\n",
    "bayes_val_y = y_val\n",
    "\n",
    "\n",
    "print(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898555c",
   "metadata": {},
   "source": [
    "# Defining the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data info\n",
    "print('Number of training data:' , len(x_train))\n",
    "print('Number of testing data:' , len(x_val))\n",
    "\n",
    "# define class labels\n",
    "class_names = ['background', 'down', 'go', 'left', 'no',\n",
    "               'off', 'on', 'right', 'stop', 'unknown','up','yes']\n",
    "num_classes = 12 # number of classes\n",
    "\n",
    "# display example image\n",
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "#plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84b45c",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising image data\n",
    "x_train = x_train/255.0\n",
    "x_val = x_val/255.0\n",
    "\n",
    "\n",
    "# display some sample images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    index = np.where(y_train[i] == 1)[0]\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[index[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c7022",
   "metadata": {},
   "source": [
    "# Model 1 (Convolutional Neural Networks)\n",
    "\n",
    "\n",
    "The model is build total of 12 layers in which convolutional layer , Batch normalization layer , Activation layer and Max Pooling layer are of 4 sets. Tuning the Number of filters to , and finally, validation accuracy of ' -- ' with number of filters for the convolutional layer are set to '64',Batch size to '16' optimizer learning rate to '0.004' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model averaging for model1\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "# define number of samples\n",
    "nsamples = len(x_train)\n",
    "# create data index\n",
    "data_index = list(range(1,nsamples))\n",
    "# create random index using sampling with replacement\n",
    "idx = random.choices(data_index, k=nsamples)\n",
    "# initialise data set 1\n",
    "x1 = np.zeros([nsamples,98,50,1])\n",
    "y1 = np.zeros([nsamples,12])\n",
    "# resample training data with replacement\n",
    "for i in range(nsamples):\n",
    "  x1[i] = x_train[idx[i],:,:,:]\n",
    "  y1[i] = y_train[idx[i],:]\n",
    "\n",
    "# Model 1 - Baseline Model(Convolutional Neural Network)\n",
    "\n",
    "\n",
    "# number of convolutional filters\n",
    "num_filters = 64\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(98,50,1)))\n",
    "\n",
    "for i in range(4):\n",
    "  model.add(Conv2D(num_filters, kernel_size =(3, 3), padding='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size =(2, 2), strides=(2, 2), padding= 'same'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(num_filters, kernel_size =(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size =(12, 1), strides=(1, 1), padding= 'same'))\n",
    "\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Softmax())\n",
    "\n",
    "# set the optimization options and compile the model\n",
    "opt = optimizers.Adam(learning_rate=0.004)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# print out the model summary\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a43dc3",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model.fit(x1, y1, batch_size=16, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d517f9",
   "metadata": {},
   "source": [
    "# Installing scikeras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e61af",
   "metadata": {},
   "source": [
    "# Model 2 - 3x2 Grid Search (Convolutional Neural Networks)\n",
    "\n",
    "1. For batch_size=128,num_layers=[2,3,4],num_filters=[32, 64]validation accuracy is 0.2784\n",
    "2. For batch_size=64,num_layers=[2,3,4],num_filters=[32, 64]validation accuracy is 0.5628\n",
    "3. For batch_size=32,num_layers=[2,3,4],num_filters=[32, 64]validation accuracy is 0.6114\n",
    "4. For batch_size=32,num_layers=[2,3,4],num_filters=[32, 64]validation accuracy is 0.6285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Model averaging for Model 2\n",
    "import numpy as np\n",
    "import random\n",
    "# define number of samples\n",
    "nsamples = len(x_train)\n",
    "# create data index\n",
    "data_index = list(range(1,nsamples))\n",
    "# create random index using sampling with replacement\n",
    "idx = random.choices(data_index, k=nsamples)\n",
    "# initialise data set 1\n",
    "x2 = np.zeros([nsamples,98,50,1])\n",
    "y2 = np.zeros([nsamples,12])\n",
    "# resample training data with replacement\n",
    "for i in range(nsamples):\n",
    "  x2[i] = x_train[idx[i],:,:,:]\n",
    "  y2[i] = y_train[idx[i],:]\n",
    "\n",
    "\n",
    "# Model 2 - Grid Search \n",
    "\n",
    "def create_model(num_layers,num_filters):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(Input(shape=(98,50,1)))\n",
    "    for i in range(num_layers-1):\n",
    "        grid_model.add(Conv2D(num_filters, kernel_size=(3, 3), padding='same'))\n",
    "        grid_model.add(BatchNormalization())\n",
    "        grid_model.add(Activation('relu'))\n",
    "        grid_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    grid_model.add(Conv2D(num_filters, kernel_size=(3, 3), padding='same'))\n",
    "    grid_model.add(BatchNormalization())\n",
    "    grid_model.add(Activation('relu'))\n",
    "\n",
    "    grid_model.add(MaxPooling2D(pool_size=(12, 1), strides=(1, 1), padding='same'))\n",
    "\n",
    "    grid_model.add(Flatten())\n",
    "\n",
    "    grid_model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "\n",
    "    # Set the optimization options and compile the model\n",
    "    opt = optimizers.Adam(learning_rate=0.004)\n",
    "    grid_model.compile( optimizer=opt,loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return grid_model\n",
    "\n",
    "\n",
    "# Create KerasClassifier\n",
    "# build_fn\n",
    "grid_model = KerasClassifier(model=create_model, epochs=10, batch_size=32, verbose=1,num_layers=[3, 4, 5, 6],num_filters=[32, 64, 128, 256])\n",
    "\n",
    "# Define parameters for grid search\n",
    "param_grid = {\n",
    "    'num_layers':[3, 4, 5, 6],\n",
    "    'num_filters': [32, 64, 128, 256]\n",
    "}\n",
    "\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid = GridSearchCV(estimator=grid_model, param_grid=param_grid,n_jobs=1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "model2 = grid.fit(x2, y2,validation_data=(x_val, y_val))\n",
    "# grid.predict(x_val)\n",
    "# first, output the best performing parameters\n",
    "print(grid.best_params_)\n",
    "# output is in\n",
    "df = pd.DataFrame(grid.cv_results_)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e97659",
   "metadata": {},
   "source": [
    "# Model 3 -  Bayesian optimization ( Convolutional Neural Network )\n",
    "\n",
    "With Bayesian optimization tried tuning the parameters and got the below accuracies:\n",
    "1. Batch size = 16, hidden layers =[2,3,4], Filters = [32,64] the validation accuracy is 0.7612.\n",
    "2. Batch size = 32, hidden layers =[2,3,4], Filters = [32,64] the validation accuracy is 0.7463.\n",
    "3. Batch size = 32, hidden layers =[2,3,4], Filters = [64,128] the validation accuracy is 0.7214\n",
    "5. Batch size = 128,  hidden layers =[2,3,4], Filters =[32,64] the validation accuracy is 0.4030\n",
    "6. Batch size = 64,  hidden layers =[2,3,4], Filters =[32,64] the validation accuracy is 0.4876\n",
    "\n",
    "Finally, Batch size = 32,  hidden layers =[2,3,4], Filters =[32,64] the validation accuracy is 0.7711 has good validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/hyperopt/hyperopt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c591c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# Model averaging for Model 2\n",
    "import numpy as np\n",
    "import random\n",
    "# define number of samples\n",
    "nsamples = len(x_train)\n",
    "# create data index\n",
    "data_index = list(range(1,nsamples))\n",
    "# create random index using sampling with replacement\n",
    "idx = random.choices(data_index, k=nsamples)\n",
    "# initialise data set 1\n",
    "x3 = np.zeros([nsamples,98,50,1])\n",
    "y3 = np.zeros([nsamples,12])\n",
    "# resample training data with replacement\n",
    "for i in range(nsamples):\n",
    "  x3[i] = x_train[idx[i],:,:,:]\n",
    "  y3[i] = y_train[idx[i],:]\n",
    "\n",
    "\n",
    "# Model 3 - Bayesian Optimization  \n",
    "\n",
    "\n",
    "space = {'num_hidden_layers': hp.choice('num_hidden_layers', [3, 4, 5, 6]),\n",
    "'num_hidden_units': hp.choice('num_hidden_units', [32, 64, 128, 256]),}\n",
    "# Define the objective function to minimize\n",
    "\n",
    "def objective(params):\n",
    "# Build the Keras model with the given hyperparameters\n",
    "  bayes_model = Sequential()\n",
    "  bayes_model.add(Input(shape=(98,50,1)))\n",
    "\n",
    "  for i in range(params['num_hidden_layers']-1):\n",
    "    bayes_model.add(Conv2D(params['num_hidden_units'], kernel_size =(3, 3), padding='same'))\n",
    "    bayes_model.add(BatchNormalization())\n",
    "    bayes_model.add(Activation('relu'))\n",
    "    bayes_model.add(MaxPooling2D(pool_size =(2, 2), strides=(2, 2), padding= 'same'))\n",
    "  bayes_model.add(Conv2D(params['num_hidden_units'], kernel_size =(3, 3), padding='same'))\n",
    "  bayes_model.add(BatchNormalization())\n",
    "  bayes_model.add(Activation('relu'))\n",
    "  bayes_model.add(MaxPooling2D(pool_size =(12, 1), strides=(1, 1), padding= 'same'))\n",
    "  bayes_model.add(Flatten())\n",
    "  bayes_model.add(Dense(num_classes))\n",
    "  bayes_model.add(Softmax())\n",
    "# set the optimization options and compile the model\n",
    "  opt = optimizers.Adam(learning_rate=0.004)\n",
    "  bayes_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "# Train the model and evaluate on the test set\n",
    "  # bayes_model.fit(x_train, y_train, epochs=10, batch_size=16, verbose=0)\n",
    "  bayes_model.fit(x3, y3, batch_size=128, epochs=10, validation_split=0.1)\n",
    "  loss, accuracy = bayes_model.evaluate(x_val, y_val, verbose=0)\n",
    "  return {'loss': -accuracy, 'status': 'ok'}\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50)\n",
    "\n",
    "\n",
    "print('best values: ',best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94acc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best hyperparameters\n",
    "# best_num_hidden_layers = space['num_hidden_layers'][best['num_hidden_layers']]\n",
    "# best_num_hidden_units = space['num_hidden_units'][best['num_hidden_units']]\n",
    "best_num_hidden_layers = best['num_hidden_layers']\n",
    "best_num_hidden_units = best['num_hidden_units']\n",
    "print('besnaksd',best)\n",
    "# Build the best Bayesian model using the best hyperparameters\n",
    "best_bayes_model = Sequential()\n",
    "best_bayes_model.add(Input(shape=(98, 50, 1)))\n",
    "\n",
    "for _ in range(best_num_hidden_layers - 1):\n",
    "    best_bayes_model.add(Conv2D(best_num_hidden_units, kernel_size=(3, 3), padding='same'))\n",
    "    best_bayes_model.add(BatchNormalization())\n",
    "    best_bayes_model.add(Activation('relu'))\n",
    "    best_bayes_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "best_bayes_model.add(Conv2D(best_num_hidden_units, kernel_size=(3, 3), padding='same'))\n",
    "best_bayes_model.add(BatchNormalization())\n",
    "best_bayes_model.add(Activation('relu'))\n",
    "best_bayes_model.add(MaxPooling2D(pool_size=(12, 1), strides=(1, 1), padding='same'))\n",
    "best_bayes_model.add(Flatten())\n",
    "best_bayes_model.add(Dense(num_classes))\n",
    "best_bayes_model.add(Softmax())\n",
    "\n",
    "# Compile the model\n",
    "opt = optimizers.Adam(learning_rate=0.004)\n",
    "best_bayes_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06171f35",
   "metadata": {},
   "source": [
    "# Model averaging Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Step 1: Train multiple models on random splits of the data\n",
    "models = [model1,model2,model3]\n",
    "\n",
    "\n",
    "# Step 2: Predict with each trained model\n",
    "predictions = []\n",
    "for model in models:\n",
    "    y_pred = model.predict(x_val)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Step 3: Take the majority class prediction (use the 'mode')\n",
    "ensemble_predictions = mode(predictions, axis=0)[0]\n",
    "\n",
    "# Step 4: Evaluate the prediction accuracy\n",
    "ensemble_accuracy = accuracy_score(y_val, ensemble_predictions)\n",
    "\n",
    "# Step 5: Benchmark/compare to a single model and see if the accuracy is better\n",
    "# Evaluate the accuracy of a single model (e.g., the first model in the list)\n",
    "single_model_accuracy = accuracy_score(y_val, predictions[0])\n",
    "\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "print(\"Single Model Accuracy:\", single_model_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbbe9a",
   "metadata": {},
   "source": [
    "# Ploting the accuracy and loss over training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec434eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(model1.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model1.history['accuracy'])\n",
    "plt.plot(model1.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model1.history['loss'])\n",
    "plt.plot(model1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243bd84",
   "metadata": {},
   "source": [
    "# Print out the accuracy on the independent Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the accuracy on independent test data\n",
    "# score = model.evaluate(test_img, test_labels, verbose=0)\n",
    "score1 = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Test accuracy:\", score1[1])\n",
    "plt.imshow(x_val[0])\n",
    "\n",
    "#Model 2\n",
    "\n",
    "score2 = model2.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Test accuracy:\", score2[1])\n",
    "plt.imshow(x_val[0])\n",
    "\n",
    "#Model 3\n",
    "\n",
    "score3 = model3.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Test accuracy:\", score3[1])\n",
    "plt.imshow(x_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a28663",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain model predictions and convert softmax outputs 0-1 to integer class label predictions\n",
    "# Yhat = model.predict(test_img)\n",
    "Yhat = model.predict(x_val)                    # predict model outputs on validation data as softmax outputs of probability of each class\n",
    "Yhat_integer = np.argmax(Yhat, axis=1)            # obtain the most likely class prediction as the argument of the max softmax output\n",
    "# Y_test_integer = np.argmax(test_labels, axis=1)\n",
    "Y_test_integer = np.argmax(y_val, axis=1)   # obtain the true class as an integer\n",
    "\n",
    "# calculate and plot confusion matrix\n",
    "cm = confusion_matrix(Y_test_integer, Yhat_integer , normalize=\"pred\")    # calculate the confusion matrix\n",
    "plt.figure(2).set_figwidth(15)                                            # setup new figure\n",
    "sns.heatmap(cm/np.sum(cm), annot=True, fmt=\".2%\", cmap=\"Blues\",)          # plot the confusion matrix using the sns package\n",
    "plt.title(\"Confusion Matrix\", fontsize = 12)                              # title\n",
    "plt.xlabel(\"Predicted Class\", fontsize = 12)                              # xlabel\n",
    "plt.ylabel(\"True Class\", fontsize = 12)                                   # ylabel\n",
    "plt.show()                                                                # show plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
